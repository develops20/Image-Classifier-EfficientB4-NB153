{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104830,"databundleVersionId":12628243,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, models\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# Configuration\nclass Config:\n    # Paths - adjust these based on your Kaggle dataset structure\n    DATA_PATH = '/kaggle/input/sheep-classification-challenge-2025/Sheep Classification Images'\n    TRAIN_DIR = os.path.join(DATA_PATH, 'train')\n    TEST_DIR = os.path.join(DATA_PATH, 'test')\n    TRAIN_CSV = os.path.join(DATA_PATH, 'train_labels.csv')\n    \n    # Model parameters\n    IMG_SIZE = 224\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 30\n    LEARNING_RATE = 1e-4\n    NUM_CLASSES = 7\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Early stopping parameters\n    EARLY_STOPPING_PATIENCE = 5\n    MIN_DELTA = 0.001  # Minimum improvement to consider as progress\n    \n    # Breeds\n    BREEDS = ['Naeimi', 'Najdi', 'Harri', 'Goat', 'Sawakni', 'Roman', 'Barbari']\n    \n    # New: Model name\n    MODEL_NAME = 'efficientnet_b4' # <--- ADD THIS LINE\n\nconfig = Config()\n\nprint(f\"Using device: {config.DEVICE}\")\nprint(f\"Available breeds: {config.BREEDS}\")\n\n# Data Loading and Exploration\ndef load_and_explore_data():\n    \"\"\"Load and explore the training data\"\"\"\n    print(\"Loading training data...\")\n    train_df = pd.read_csv(config.TRAIN_CSV)\n    \n    print(f\"Training data shape: {train_df.shape}\")\n    print(f\"Unique breeds: {train_df['label'].unique()}\")\n    print(\"\\nBreed distribution:\")\n    print(train_df['label'].value_counts())\n    \n    # Visualize distribution\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    train_df['label'].value_counts().plot(kind='bar')\n    plt.title('Breed Distribution')\n    plt.xlabel('Breed')\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    \n    plt.subplot(1, 2, 2)\n    train_df['label'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n    plt.title('Breed Distribution (Percentage)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return train_df\n\n# Custom Dataset Class\nclass SheepDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None, is_test=False):\n        self.df = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n        self.is_test = is_test\n        \n        if not is_test:\n            self.label_encoder = LabelEncoder()\n            self.labels = self.label_encoder.fit_transform(self.df['label'])\n            self.label_to_idx = {label: idx for idx, label in enumerate(self.label_encoder.classes_)}\n            self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx]['filename']\n        img_path = os.path.join(self.img_dir, filename)\n        \n        # Load image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            # Return a blank image if loading fails\n            image = Image.new('RGB', (config.IMG_SIZE, config.IMG_SIZE), (0, 0, 0))\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.is_test:\n            return image, filename\n        else:\n            label = self.labels[idx]\n            return image, label\n\n# Data Augmentation and Preprocessing\ndef get_transforms():\n    \"\"\"Define data augmentation and preprocessing transforms\"\"\"\n    train_transform = transforms.Compose([\n        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    return train_transform, val_transform\n\ndef get_tta_transforms():\n    \"\"\"Define TTA transforms for test-time augmentation.\"\"\"\n    tta_transforms = [\n        transforms.Compose([\n            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ]),\n        transforms.Compose([\n            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n            transforms.RandomHorizontalFlip(p=1.0), # Horizontal Flip\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ]),\n        transforms.Compose([\n            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n            transforms.RandomRotation(degrees=5), # Slight rotation\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ]),\n        # Add more TTA variations if desired, e.g.,\n        # transforms.Compose([\n        #     transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n        #     transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        #     transforms.ToTensor(),\n        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        # ]),\n    ]\n    return tta_transforms\n\n\n\n\n Model Definition\n# class SheepClassifier(nn.Module):\n#     def __init__(self, num_classes=7, model_name='efficientnet_b4'):\n#         super(SheepClassifier, self).__init__()\n        \n#         if model_name == 'efficientnet_b4':\n#             self.backbone = models.efficientnet_b3(pretrained=True)\n#             in_features = self.backbone.classifier[1].in_features\n#             self.backbone.classifier = nn.Sequential(\n#                 nn.Dropout(0.3),\n#                 nn.Linear(in_features, 512),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),\n#                 nn.Linear(512, num_classes)\n#             )\n#         elif model_name == 'resnet50':\n#             self.backbone = models.resnet50(pretrained=True)\n#             in_features = self.backbone.fc.in_features\n#             self.backbone.fc = nn.Sequential(\n#                 nn.Dropout(0.3),\n#                 nn.Linear(in_features, 512),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),\n#                 nn.Linear(512, num_classes)\n#             )\n        \n#     def forward(self, x):\n#         return self.backbone(x)\n\nclass SheepClassifier(nn.Module):\n    def __init__(self, num_classes=7, model_name='efficientnet_b4'): # Change default model_name, though it will be overridden by config.MODEL_NAME\n        super(SheepClassifier, self).__init__()\n\n        # Use the passed model_name from Config\n        if model_name == 'efficientnet_b4': # <--- CHANGED FROM 'efficientnet_b3'\n            # Correct way to load pre-trained weights in torchvision >= 0.13\n            self.backbone = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1) # <--- UPDATED\n            in_features = self.backbone.classifier[1].in_features\n            self.backbone.classifier = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, 512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, num_classes)\n            )\n        elif model_name == 'efficientnet_b3': # Keep B3 as an option or remove if not needed\n            self.backbone = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1) # <--- UPDATED\n            in_features = self.backbone.classifier[1].in_features\n            self.backbone.classifier = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, 512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, num_classes)\n            )\n        elif model_name == 'resnet50':\n            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1) # <--- UPDATED\n            in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, 512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, num_classes)\n            )\n\n    def forward(self, x):\n        return self.backbone(x)\n\n# Early Stopping Class\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_score = None\n        self.counter = 0\n        self.best_weights = None\n        self.early_stop = False\n        \n    def __call__(self, val_score, model):\n        if self.best_score is None:\n            self.best_score = val_score\n            self.save_checkpoint(model)\n        elif val_score < self.best_score + self.min_delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.restore_best_weights:\n                    print('Restoring best weights')\n                    model.load_state_dict(self.best_weights)\n        else:\n            self.best_score = val_score\n            self.save_checkpoint(model)\n            self.counter = 0\n            \n    def save_checkpoint(self, model):\n        '''Saves model when validation score improves.'''\n        self.best_weights = model.state_dict().copy()\n\n# Training Function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, early_stopping=None):\n    \"\"\"Train the model with validation\"\"\"\n    best_f1 = 0.0\n    best_model_state = None\n    train_losses = []\n    val_losses = []\n    val_f1_scores = []\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(config.DEVICE), target.to(config.DEVICE)\n            \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            if batch_idx % 20 == 0:\n                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n        \n        avg_train_loss = running_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        all_preds = []\n        all_targets = []\n        \n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(config.DEVICE), target.to(config.DEVICE)\n                output = model(data)\n                val_loss += criterion(output, target).item()\n                \n                _, predicted = torch.max(output.data, 1)\n                all_preds.extend(predicted.cpu().numpy())\n                all_targets.extend(target.cpu().numpy())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        \n        # Calculate F1 score\n        f1 = f1_score(all_targets, all_preds, average='macro')\n        val_f1_scores.append(f1)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'  Train Loss: {avg_train_loss:.4f}')\n        print(f'  Val Loss: {avg_val_loss:.4f}')\n        print(f'  Val F1 Score: {f1:.4f}')\n        \n        # Save best model based on F1 score\n        if f1 > best_f1:\n            best_f1 = f1\n            best_model_state = model.state_dict().copy()\n            print(f'  New best F1 score: {best_f1:.4f}')\n\n        # Early stopping check\n        if early_stopping is not None:\n            early_stopping(f1, model)\n            if early_stopping.early_stop:\n                print(f\"Early stopping triggered at epoch {epoch+1}\")\n                print(f\"Best F1 score was: {early_stopping.best_score:.4f}\")\n                break\n        \n        scheduler.step()\n        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n        print('-' * 50)\n    \n    # Load best model (either from early stopping or best F1)\n    if early_stopping is not None and early_stopping.best_weights is not None:\n        model.load_state_dict(early_stopping.best_weights)\n        final_f1 = early_stopping.best_score\n    else:\n        model.load_state_dict(best_model_state)\n        final_f1 = best_f1\n    \n    return model, train_losses, val_losses, val_f1_scores, final_f1\n\n# Prediction Function\n# def make_predictions(model, test_loader, label_encoder):\n#     \"\"\"Make predictions on test set\"\"\"\n#     model.eval()\n#     predictions = []\n#     filenames = []\n    \n#     with torch.no_grad():\n#         for data, filename_batch in test_loader:\n#             data = data.to(config.DEVICE)\n#             outputs = model(data)\n#             _, predicted = torch.max(outputs, 1)\n            \n#             # Convert predictions back to breed names\n#             pred_labels = label_encoder.inverse_transform(predicted.cpu().numpy())\n            \n#             predictions.extend(pred_labels)\n#             filenames.extend(filename_batch)\n    \n#     return filenames, predictions\n\ndef make_predictions(model, test_df, img_dir, label_encoder): # <--- Changed parameter: now takes test_df directly\n    \"\"\"Make predictions on test set with Test-Time Augmentation (TTA)\"\"\"\n    model.eval()\n    tta_transforms = get_tta_transforms() # Get the list of TTA transforms\n    \n    predictions = []\n    filenames = []\n\n    # Process each image individually for TTA\n    for idx in tqdm(range(len(test_df)), desc=\"Generating TTA Predictions\"):\n        filename = test_df.iloc[idx]['filename']\n        img_path = os.path.join(img_dir, filename)\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            image = Image.new('RGB', (config.IMG_SIZE, config.IMG_SIZE), (0, 0, 0))\n\n        # Collect probabilities from all TTA transforms\n        tta_outputs = []\n        with torch.no_grad():\n            for tta_transform in tta_transforms:\n                augmented_image = tta_transform(image).unsqueeze(0).to(config.DEVICE)\n                output = model(augmented_image)\n                tta_outputs.append(torch.softmax(output, dim=1)) # Get probabilities\n\n        # Average probabilities across TTA transforms\n        avg_output = torch.mean(torch.cat(tta_outputs, dim=0), dim=0)\n        \n        # Get the final predicted class\n        _, predicted_class_id = torch.max(avg_output, 0) # Use 0 because avg_output is now 1D\n        \n        # Convert prediction back to breed name\n        pred_label = label_encoder.inverse_transform([predicted_class_id.item()])[0]\n        \n        predictions.append(pred_label)\n        filenames.append(filename)\n            \n    return filenames, predictions\n\n# Visualization Functions\ndef plot_training_history(train_losses, val_losses, val_f1_scores):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    # Plot losses\n    axes[0].plot(train_losses, label='Train Loss')\n    axes[0].plot(val_losses, label='Validation Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Plot F1 scores\n    axes[1].plot(val_f1_scores, label='Validation F1', color='green')\n    axes[1].set_title('Validation F1 Score')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('F1 Score')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    # Plot learning rate (if using scheduler)\n    axes[2].set_title('Model Performance Summary')\n    axes[2].text(0.1, 0.8, f'Best Validation F1: {max(val_f1_scores):.4f}', fontsize=12)\n    axes[2].text(0.1, 0.6, f'Final Train Loss: {train_losses[-1]:.4f}', fontsize=12)\n    axes[2].text(0.1, 0.4, f'Final Val Loss: {val_losses[-1]:.4f}', fontsize=12)\n    axes[2].set_xlim(0, 1)\n    axes[2].set_ylim(0, 1)\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef visualize_sample_predictions(model, dataset, label_encoder, num_samples=8):\n    \"\"\"Visualize sample predictions\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.ravel()\n    \n    # Get random samples\n    indices = np.random.choice(len(dataset), num_samples, replace=False)\n    \n    with torch.no_grad():\n        for i, idx in enumerate(indices):\n            image, true_label = dataset[idx]\n            \n            # Make prediction\n            image_batch = image.unsqueeze(0).to(config.DEVICE)\n            output = model(image_batch)\n            _, predicted = torch.max(output, 1)\n            \n            # Convert to breed names\n            true_breed = label_encoder.inverse_transform([true_label])[0]\n            pred_breed = label_encoder.inverse_transform(predicted.cpu().numpy())[0]\n            \n            # Denormalize image for display\n            img_display = image.permute(1, 2, 0).numpy()\n            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img_display = np.clip(img_display, 0, 1)\n            \n            # Plot\n            axes[i].imshow(img_display)\n            axes[i].set_title(f'True: {true_breed}\\nPred: {pred_breed}', \n                            color='green' if true_breed == pred_breed else 'red')\n            axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main Pipeline\ndef main():\n    \"\"\"Main pipeline execution\"\"\"\n    print(\"=== Sheep Classification Pipeline ===\")\n    print(f\"Competition: Eid Al-Adha 2025 - Sheep Classification Challenge\")\n    print(f\"Evaluation Metric: Macro F1 Score\")\n    print(\"=\" * 50)\n    \n    # 1. Load and explore data\n    train_df = load_and_explore_data()\n    \n    # 2. Calculate class weights for handling imbalanced dataset\n    print(\"\\nCalculating class weights for imbalanced dataset...\")\n    \n    # Create temporary dataset to fit label encoder on full training data\n    temp_dataset = SheepDataset(train_df, config.TRAIN_DIR, None, is_test=False)\n    unique_labels = temp_dataset.label_encoder.classes_  # Get sorted list of all unique labels\n    train_labels_numerical = temp_dataset.labels  # Get numerical labels from full dataframe\n    \n    # Calculate balanced class weights\n    class_weights_array = compute_class_weight(\n        class_weight='balanced',  # 'balanced' automatically computes inverse weights\n        classes=np.unique(train_labels_numerical),  # Use the numerical labels\n        y=train_labels_numerical\n    )\n    class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float).to(config.DEVICE)\n    \n    print(\"\\nCalculated Class Weights:\")\n    for i, weight in enumerate(class_weights_array):\n        print(f\"  {temp_dataset.idx_to_label[i]}: {weight:.4f}\")\n    \n    print(f\"\\nClass weight tensor shape: {class_weights_tensor.shape}\")\n    print(f\"Weights will be applied to CrossEntropyLoss\")\n    \n    # 3. Prepare transforms\n    train_transform, val_transform = get_transforms()\n    \n    # 3. Create train/validation split\n    train_split, val_split = train_test_split(\n        train_df, \n        test_size=0.2, \n        stratify=train_df['label'], \n        random_state=42\n    )\n    \n    print(f\"Train split size: {len(train_split)}\")\n    print(f\"Validation split size: {len(val_split)}\")\n    \n    # 4. Create datasets and dataloaders\n    train_dataset = SheepDataset(train_split, config.TRAIN_DIR, train_transform)\n    val_dataset = SheepDataset(val_split, config.TRAIN_DIR, val_transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    print(f\"Number of training batches: {len(train_loader)}\")\n    print(f\"Number of validation batches: {len(val_loader)}\")\n    \n    # 6. Initialize model\n    model = SheepClassifier(num_classes=config.NUM_CLASSES, model_name=config.MODEL_NAME) # Ability to easy switch between models. \n    model = model.to(config.DEVICE)\n    \n    # 6. Define loss function, optimizer, and scheduler\n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS)\n    \n    print(f\"Model: EfficientNet-B3\")\n    print(f\"Optimizer: AdamW (lr={config.LEARNING_RATE})\")\n    print(f\"Scheduler: CosineAnnealingLR\")\n    print(f\"Loss function: CrossEntropyLoss\")\n    \n    # 8. Train model with early stopping\n    print(\"\\nStarting training with early stopping...\")\n    early_stopping = EarlyStopping(patience=config.EARLY_STOPPING_PATIENCE, \n                                   min_delta=config.MIN_DELTA, \n                                   restore_best_weights=True)\n    \n    model, train_losses, val_losses, val_f1_scores, best_f1 = train_model(\n        model, train_loader, val_loader, criterion, optimizer, scheduler, \n        config.NUM_EPOCHS, early_stopping\n    )\n    \n    # 9. Plot training history\n    plot_training_history(train_losses, val_losses, val_f1_scores)\n    \n    # 10. Visualize sample predictions\n    visualize_sample_predictions(model, val_dataset, train_dataset.label_encoder)\n    \n    # 11. Prepare test dataset\n    print(\"\\nPreparing test predictions...\")\n    test_files = [f for f in os.listdir(config.TEST_DIR) if f.endswith('.jpg')]\n    test_df = pd.DataFrame({'filename': test_files})\n    \n    test_dataset = SheepDataset(test_df, config.TEST_DIR, val_transform, is_test=True)\n    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    # 11. Make predictions function (now handles image loading internally)\n    filenames, predictions = make_predictions(model, test_df, config.TEST_DIR, train_dataset.label_encoder)\n    \n    # 13. Create submission file\n    submission_df = pd.DataFrame({\n        'filename': filenames,\n        'label': predictions\n    })\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(f\"\\nSubmission file created: submission.csv\")\n    print(f\"Shape: {submission_df.shape}\")\n    print(f\"Sample predictions:\")\n    print(submission_df.head(10))\n    \n    # 14. Display class distribution in predictions\n    print(f\"\\nPrediction distribution:\")\n    print(submission_df['label'].value_counts())\n    \n    print(f\"\\nFinal Results:\")\n    print(f\"Best Validation F1 Score: {best_f1:.4f}\")\n    print(f\"Total test predictions: {len(predictions)}\")\n    print(f\"Class weights successfully applied to handle imbalanced dataset\")\n    \n    return model, submission_df, best_f1, class_weights_tensor\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    model, submission_df, best_f1, class_weights = main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T03:58:23.266024Z","iopub.execute_input":"2025-06-12T03:58:23.266708Z","iopub.status.idle":"2025-06-12T04:00:08.964242Z","shell.execute_reply.started":"2025-06-12T03:58:23.266679Z","shell.execute_reply":"2025-06-12T04:00:08.963410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}